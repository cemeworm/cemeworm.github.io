<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/blog_logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://alfredchen.cn').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.7.1',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: 'CMBVS1RI6H',
      apiKey: '0af47ecc6edcfb261c0c076706e147bd',
      indexName: 'blog',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="本书是一本机器学习入门书，在不涉及对大量数学模型和复杂编程知识的情况下整合并实践基于python语言的程序库，如scikit-learn、pandas、NLTK、gensim、XGBoost、TensorFlow等，并且介绍了线性分类器、逻辑斯蒂模型、朴素贝叶斯分类器等机器学习经典模型及优化办法，最后介绍了机器学习实战平台kaggle并给出实例。">
<meta property="og:type" content="article">
<meta property="og:title" content="《Python机器学习及实战》笔记">
<meta property="og:url" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Alfred Chen">
<meta property="og:description" content="本书是一本机器学习入门书，在不涉及对大量数学模型和复杂编程知识的情况下整合并实践基于python语言的程序库，如scikit-learn、pandas、NLTK、gensim、XGBoost、TensorFlow等，并且介绍了线性分类器、逻辑斯蒂模型、朴素贝叶斯分类器等机器学习经典模型及优化办法，最后介绍了机器学习实战平台kaggle并给出实例。">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/1.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/2.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/3.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/4.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/5.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/6.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/7.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/8.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/9.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/10.jpg">
<meta property="og:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/11.jpg">
<meta property="article:published_time" content="2019-03-14T14:29:34.000Z">
<meta property="article:modified_time" content="2020-02-03T06:03:22.083Z">
<meta property="article:author" content="Alfred Chen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/1.jpg">

<link rel="canonical" href="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>《Python机器学习及实战》笔记 | Alfred Chen</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b959ef0097ea2abdf23b5dd4db0f140a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Alfred Chen</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input" id="search-input"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

  
</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Alfred Chen">
      <meta itemprop="description" content="对不合理之处保持敏感">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Alfred Chen">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《Python机器学习及实战》笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-03-14 22:29:34" itemprop="dateCreated datePublished" datetime="2019-03-14T22:29:34+08:00">2019-03-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%A7%E5%93%81%E6%96%B9%E6%B3%95/" itemprop="url" rel="index">
                    <span itemprop="name">产品方法</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%A7%E5%93%81%E6%96%B9%E6%B3%95/AI%E4%BA%A7%E5%93%81/" itemprop="url" rel="index">
                    <span itemprop="name">AI产品</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本书是一本机器学习入门书，在不涉及对大量数学模型和复杂编程知识的情况下整合并实践基于python语言的程序库，如scikit-learn、pandas、NLTK、gensim、XGBoost、TensorFlow等，并且介绍了线性分类器、逻辑斯蒂模型、朴素贝叶斯分类器等机器学习经典模型及优化办法，最后介绍了机器学习实战平台kaggle并给出实例。</p>
<a id="more"></a>

<h1 id="第一章-简介篇"><a href="#第一章-简介篇" class="headerlink" title="第一章 简介篇"></a>第一章 简介篇</h1><h2 id="机器学习综述"><a href="#机器学习综述" class="headerlink" title="机器学习综述"></a>机器学习综述</h2><p>如果一个程序在使用既有的经验执行某类任务的过程中被认定为是“具备学习能力的”，那么它一定需要展现出：利用现有的经验，不断改善其完成既定任务的性能的特质</p>
<h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><h3 id="监督学习：关注对事物未知表现的预测"><a href="#监督学习：关注对事物未知表现的预测" class="headerlink" title="监督学习：关注对事物未知表现的预测"></a>监督学习：关注对事物未知表现的预测</h3><ul>
<li><p>分类问题：对其所在的类别进行预测。类别既是<strong>离散</strong>的，同时也是<strong>预先知道数量</strong>的（如根据一个人的身高、体重和三维等数据预测其性别）</p>
</li>
<li><p>回归问题：预测的目标往往是<strong>连续</strong>的，如根据房屋的面积、地理位置、建筑年代等进行销售价格的预测</p>
</li>
</ul>
<h3 id="无监督学习：倾向于对事物本身特性的分析"><a href="#无监督学习：倾向于对事物本身特性的分析" class="headerlink" title="无监督学习：倾向于对事物本身特性的分析"></a>无监督学习：倾向于对事物本身特性的分析</h3><ul>
<li><p>数据降维：对事物的特性进行压缩和筛选，如识别图像中人脸的任务通常会对图像进行降维，保留最具有区分度的像素组合</p>
</li>
<li><p>聚类问题：依赖于数据的相似性，把相似的数据样本划分为一个簇（区别于分类在大多数情况下不会预先知道簇的数量和每个簇的具体含义），如电子商务网站根据用户信息和购买行为进行聚类分析，一旦找到数量多且背景相似的客户群，便可以针对他们的兴趣投放广告和促销信息</p>
</li>
</ul>
<h2 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h2><ul>
<li><p>监督学习：我们拥有的经验包括特征和标记，特征向量描述数据样本，标记的表现形式则取决于监督学习的种类</p>
</li>
<li><p>无监督学习：更加适合对数据结构的分析，数据量大</p>
</li>
<li><p>数据类型：从原始数据到特征向量转化的过程中也会遭遇多种数据类型：类别型是特征、数值型特征、缺失的数据</p>
</li>
<li><p>训练集：既有特征，同时也带有标记的数据集</p>
</li>
</ul>
<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><ul>
<li><p>性能：评价所完成任务质量的指标</p>
</li>
<li><p>测试集：为了评价学习模型完成任务的质量，我们需要具备相同特征的数据，并将模型的预测结果通相对应的正确答案进行比对，这样的数据集成为测试集</p>
</li>
<li><p>指标：分类问题——准确性，回归问题——衡量预测值与实际值之间的偏差大小</p>
</li>
<li><p>模型的参数：分类器需要通过学习从训练数据中得到</p>
</li>
</ul>
<h1 id="python编程库"><a href="#python编程库" class="headerlink" title="python编程库"></a>python编程库</h1><h2 id="python用于机器学习的优势"><a href="#python用于机器学习的优势" class="headerlink" title="python用于机器学习的优势"></a>python用于机器学习的优势</h2><ul>
<li><p>方便调试的解释型语言</p>
</li>
<li><p>跨平台执行作业</p>
</li>
<li><p>广泛的应用编程接口</p>
</li>
<li><p>丰富完备的开源工具包</p>
</li>
</ul>
<h2 id="python编程库-1"><a href="#python编程库-1" class="headerlink" title="python编程库"></a>python编程库</h2><ul>
<li><p>numpy&amp;scipy：提供高级的数学运算机制，高效的向量和矩阵运算功能</p>
</li>
<li><p>matplotlib：数据分析和可视化</p>
</li>
<li><p>scikit-learn：封装了大量经典以及最新的机器学习模型</p>
</li>
<li><p>pandas：便于数据读写、清洗、填充以及分析</p>
</li>
<li><p>Anaconda：集成平台</p>
</li>
</ul>
<h1 id="第二章-基础篇"><a href="#第二章-基础篇" class="headerlink" title="第二章 基础篇"></a>第二章 基础篇</h1><h2 id="监督学习经典模型"><a href="#监督学习经典模型" class="headerlink" title="监督学习经典模型"></a>监督学习经典模型</h2><h3 id="监督学习的流程"><a href="#监督学习的流程" class="headerlink" title="监督学习的流程"></a>监督学习的流程</h3><ul>
<li><p>准备训练数据</p>
</li>
<li><p>抽取所需要的特征，形成特征向量</p>
</li>
<li><p>把这些特征向量连同对应的标记/目标一并送入学习算法中，训练出一个预测模型</p>
</li>
<li><p>采用同样的特征抽取方法作用于新测试数据，得到用于测试的特征向量</p>
</li>
<li><p>使用预测模型对这些待测试的特征向量进行预测并得到结果</p>
</li>
</ul>
<p>​         <img src="1.jpg" alt="img">       </p>
<h2 id="分类学习"><a href="#分类学习" class="headerlink" title="分类学习"></a>分类学习</h2><ul>
<li><p>二分类和多分类问题：从两个或多个类别中选择一个作为预测结果</p>
</li>
<li><p>多标签分类问题：判断一个样本是否同时属于多个不同类别</p>
</li>
</ul>
<h3 id="线性分类器（Linear-Classifier）"><a href="#线性分类器（Linear-Classifier）" class="headerlink" title="线性分类器（Linear Classifier）"></a>线性分类器（Linear Classifier）</h3><p><strong>模型介绍：</strong></p>
<ul>
<li><p>一种假设特征与分类结果存在<strong>线性关系</strong>的模型，通过累加计算每个维度的特征与各自权重的乘积来帮助类别决策</p>
</li>
<li><p>逻辑回归模型：</p>
</li>
</ul>
<p>​         <img src="2.jpg" alt="img">       </p>
<p>​         <img src="3.jpg" alt="img">       </p>
<blockquote>
<p>1.任何模型在训练集上的表现都不一定能代表其最终在未知待测数据集上的性能，但至少要先保证模型可以被训练集优化。</p>
</blockquote>
<blockquote>
<p>2.SGA和SGD都属于梯度法迭代渐进估计参数的过程，梯度上升(SGA)用于目标最大化，梯度下降(SGD)用于目标最小化。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#创建特征列表</span><br><span class="line">column_names &#x3D; [&#39;Sample code number&#39;,&#39;Clump Thickness&#39;,&#39;Uniformity of Cell Size&#39;,&#39;Uniformity of Cell Shape&#39;,&#39;Marginal Adhesion&#39;,&#39;Single Epithelial Cell Size&#39;,&#39;Bare Nuclei&#39;,&#39;Bland Chromatin&#39;,&#39;Normal Nucleoli&#39;,&#39;Mitoses&#39;,&#39;Class&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#读取数据</span><br><span class="line">data &#x3D; pd.read_csv(&#39;https:&#x2F;&#x2F;archive.ics.uci.edu&#x2F;ml&#x2F;machine-learning-databases&#x2F;breast-cancer-wisconsin&#x2F;breast-cancer-wisconsin.data&#39;,names&#x3D;column_names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#清洗数据</span><br><span class="line">#将？替换为标准缺失值表示</span><br><span class="line">data &#x3D; data.replace(to_replace&#x3D;&#39;?&#39;,value&#x3D;np.nan)</span><br><span class="line">#丢弃带有缺失值的数据</span><br><span class="line">data&#x3D;data.dropna(how&#x3D;&#39;any&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#输出data的数据量和维度</span><br><span class="line">data.shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#分割数据</span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test &#x3D; </span><br><span class="line">#随机采样25%的数据用于测试</span><br><span class="line">train_test_split(data[column_names[1:10]],data[column_names[10]],test_size&#x3D;0.25,random_state&#x3D;33)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#查验训练样本的数量和类别分布</span><br><span class="line">y_train.value_counts()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#查验训练样本的数量和类别分布</span><br><span class="line">y_test.value_counts()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#从sklearn.preprocessing导入StandardScaler</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">#从sklearn.linear_model导入LogisticRegression（逻辑斯蒂回归）</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">#从sklearn.linear_model导入SGDClassifier（随机梯度参数）</span><br><span class="line">from sklearn.linear_model import SGDClassifier&lt;br&gt;&lt;br&gt;</span><br><span class="line">#标准化数据，保证每个维度的特征数据方差为1，均值为，使得预测结果不会被某些过大的特征值而主导（在机器学习训练之前, 先对数据预先处理一下, 取值跨度大的特征数据, &lt;br&gt;我们浓缩一下, 跨度小的括展一下, 使得他们的跨度尽量统一.）</span><br><span class="line">ss &#x3D; StandardScaler()</span><br><span class="line">X_train &#x3D; ss.fit_transform(X_train)</span><br><span class="line">X_test &#x3D; ss.transform(X_test)&lt;br&gt;&lt;br&gt;</span><br><span class="line">#初始化两种模型</span><br><span class="line">lr &#x3D; LogisticRegression()</span><br><span class="line">sgdc &#x3D; SGDClassifier()</span><br><span class="line">#调用逻辑斯蒂回归，使用fit函数训练模型参数</span><br><span class="line">lr.fit(X_train,y_train)</span><br><span class="line">#使用训练好的模型lr对x_test进行预测，结果储存在变量lr_y_predict中</span><br><span class="line">lr_y_predict &#x3D; lr.predict(X_test)</span><br><span class="line">#调用随机梯度的fit函数训练模型</span><br><span class="line">sgdc.fit(X_train,y_train)&lt;br&gt;</span><br><span class="line">#使用训练好的模型sgdc对X_test进行预测，结果储存在变量sgdc_y_predict中</span><br><span class="line">sgdc_y_predict &#x3D; sgdc.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#从sklearn.metrics导入classification_report</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"> </span><br><span class="line">#使用逻辑斯蒂回归模型自带的评分函数score获得模型在测试集上的准确性结果</span><br><span class="line">print(&#39;Accuracy of LR Classifier:&#39;,lr.score(X_test,y_test))</span><br><span class="line">#使用classification_report模块获得逻辑斯蒂模型其他三个指标的结果（召回率，精确率，调和平均数）</span><br><span class="line">print(classification_report(y_test,lr_y_predict,target_names&#x3D;[&#39;Benign&#39;,&#39;Malignant&#39;]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#使用随机梯度下降模型自带的评分函数score获得模型在测试集上的准确性结果</span><br><span class="line">print(&#39;Accuarcy of SGD Classifier:&#39;,sgdc.score(X_test,y_test))</span><br><span class="line">#使用classification_report模块获得随机梯度下降模型其他三个指标的结果</span><br><span class="line">print(classification_report(y_test,sgdc_y_predict,target_names&#x3D;[&#39;Benign&#39;,&#39;Mali</span><br></pre></td></tr></table></figure>



<p>综上，发现，LR比SGDC在测试集上有更高的准确性，因为sklearn中采用解析的方式精确计算LR的参数，而使用梯度法估计SGDC的参数</p>
<p>特点分析：</p>
<ul>
<li><p>LR model：精确解析参数，计算时间长但模型性能略高</p>
</li>
<li><p>SGDC model：随机梯度上升算法估计参数，计算时间短但模型性能略低</p>
</li>
<li><p>训练数据规模在10万量级以上的数据，考虑到时间耗用，推荐使用随机梯度算法对模型参数进行估计</p>
</li>
</ul>
<p><strong>sklearn.model_selection.train_test_split解释</strong></p>
<p>from sklearn.cross_validation import train_test_split</p>
<p>一般形式：X_train,X_test, y_train, y_test = cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)</p>
<p>参数解释：</p>
<ul>
<li><p>train_data：所要划分的样本特征集</p>
</li>
<li><p>train_target：所要划分的样本结果</p>
</li>
<li><p>test_size：样本占比，如果是整数的话就是样本的数量</p>
</li>
<li><p>random_state：是随机数的种子</p>
</li>
</ul>
<blockquote>
<p>随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。</p>
</blockquote>
<blockquote>
<p>随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：</p>
</blockquote>
<blockquote>
<p>种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。</p>
</blockquote>
<p><strong>混淆矩阵：</strong></p>
<p>二分类任务中，预测结果(predicted condition)与正确标记(true condition)之间存在4种不同的组合：</p>
<ul>
<li><p>真阳性(true positive)：预测正确的恶性肿瘤</p>
</li>
<li><p>真阴性</p>
</li>
<li><p>假阳性(false positive)：误判为恶性肿瘤</p>
</li>
<li><p>假阴性</p>
</li>
</ul>
<p><strong>性能评价指标：</strong><a href="https://alfredchen.cn/2019/03/10/%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E3%80%81F1%E5%80%BC%E8%A7%A3%E6%9E%90/">博客《准确率、精确率、召回率、F1值》</a></p>
<h3 id="支持向量机（Support-Vector-Classifier）"><a href="#支持向量机（Support-Vector-Classifier）" class="headerlink" title="支持向量机（Support Vector Classifier）"></a>支持向量机（Support Vector Classifier）</h3><p>​         <img src="4.jpg" alt="img">       </p>
<p><strong>模型介绍：</strong></p>
<ul>
<li><p>根据训练样本的分布，搜索所有可能的线性分类器中最佳的那个</p>
</li>
<li><p>决定直线位置的样本并不是所有训练数据，而是其中的两个空间间隔最小的两个不同类别的数据点，这种可以用来真正帮助决策最优线性分类模型的数据点叫做“支持向量”</p>
</li>
<li><p>逻辑斯蒂回归模型在训练过程中由于考虑了所有训练样本对参数的影响，因此不一定获得最佳的分类器</p>
</li>
</ul>
<blockquote>
<p>1.不是在所有数据集上SVM的表现一定都优于普通线性模型或其他模型，而是假设未知待测数据也如训练数据一样分布，则SVM可帮助找到最佳分类器；实际应用数据总是有偏差的。</p>
</blockquote>
<blockquote>
<p>2.上图，H1表现不佳（有分类错误）；H2与H3都表现完美。</p>
</blockquote>
<blockquote>
<p>3.但，分类模型的选取中我们需要更加关注如何最大限度为未知分布的数据集提供足够的待预测空间。如有一个黑色样本稍偏离H2，则会很可能被误判为白色，造成误差，而H3则可为样本提供更多的容忍度，故H3优于H2.</p>
</blockquote>
<p><strong>多分类：判断一个样本是否同时属于多个不同类别；将多分类看成N个二分类任务</strong>。</p>
<p>如本例的分类目标有10个类别，即0—9这10个数字，因此无法直接计算三指标。故我们逐一评估每个类别的这三指标，把所有其他类别统一看做阴性(负)样本，则创造了10个二分类任务。</p>
<p><strong>特点分析</strong>：</p>
<ul>
<li><p>可帮助在海量甚至高维度数据中筛选对预测任务最有效的少数训练样本，节省数据内存，提高模型预测性能</p>
</li>
<li><p>但计算代价高（CPU资源与计算时间）</p>
</li>
</ul>
<h3 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h3><p><strong>模型介绍：</strong></p>
<ul>
<li><p>会单独考量每一维度特征被分类的条件概率，进而综合这些概率并对其所在的特征向量做出分类预测</p>
</li>
<li><p>基本数学假设：<strong>各个维度上的特征被分类的条件概率之间是相互独立的</strong></p>
</li>
</ul>
<p>​         <img src="5.jpg" alt="img">       </p>
<p><strong>特点分析：</strong></p>
<ul>
<li><p>朴素贝叶斯模型广泛应用在互联网文本分类任务</p>
</li>
<li><p>优点：由于其较强的特征条件独立假设，使得模型预测所需估计的参数规模从幂指数量级向线性量级减少，极大节约内存消耗和计算时间</p>
</li>
<li><p>缺点：同样由于这种强假设的限制，模型训练时无法将各个特征之间的联系考量在内，使该模型在其他数据特征关联性强的分类任务上性能不佳</p>
</li>
</ul>
<h3 id="K近邻（K-Nearest-Neighbor，KNN）"><a href="#K近邻（K-Nearest-Neighbor，KNN）" class="headerlink" title="K近邻（K-Nearest Neighbor，KNN）"></a>K近邻（K-Nearest Neighbor，KNN）</h3><p><strong>模型介绍：</strong></p>
<ul>
<li><p>最简单的ML算法之一</p>
</li>
<li><p>假设有一些携带分类标记的训练样本，分布于特征空间中；蓝色、绿色样本点各自代表其类别；对一个待分类的红色测试样本点，未知其类别，按照“近朱者赤近墨者黑”的说法，我们需要寻找与这个待分类的样本在特征空间中距离最近的K个已标记样本作为参考，来帮助做出分类决策</p>
</li>
<li><p>思路:如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p>
</li>
</ul>
<p><strong>特点分析：</strong></p>
<ul>
<li><p>k近邻算法是非常直观简单的模型</p>
</li>
<li><p>是<strong>无参数模型</strong>：<strong>没有参数训练过程</strong>，即未通过任何学习算法分析数据，而只是根据测试样本在训练数据中的分布做出分类</p>
</li>
<li><p>缺点：<strong>高计算复杂度和内存消耗</strong>；平方级别的算法复杂度（每处理一个测试样本就要对所有训练样本进行遍历，逐一计算相似度、排序且选取K个最近邻训练样本的标记，进而做出分类决策）</p>
</li>
<li><p>当然也有KD-Tree这样的数据结构通过“空间换时间”思想节省KNN的决策时间</p>
</li>
</ul>
<h3 id="决策树（Decision-Tree）"><a href="#决策树（Decision-Tree）" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h3><p><strong>模型介绍：</strong></p>
<ul>
<li><strong>描述非线性关系</strong></li>
</ul>
<blockquote>
<p>LR和SVM都要求被学习的数据特征和目标之间遵照线性假设，但现实场景下这种假设不存在。如用年龄预测流感死亡率，年龄与死亡率之间不存在线性关系。</p>
</blockquote>
<blockquote>
</blockquote>
<ul>
<li><p>决策树<strong>节点(node)——数据特征</strong></p>
</li>
<li><p>各节点下的<strong>分支——特征值的分类</strong></p>
</li>
<li><p>决策树的<strong>所有叶子节点——显示模型的决策结果</strong></p>
</li>
<li><p>使用多种不同特征组合搭建多层决策树时，需<strong>考虑特征节点的选取顺序</strong>，常用的度量方式有信息熵(Information Gain)和基尼不纯性(Gini Impurity)</p>
</li>
</ul>
<h3 id="集成模型（Ensemble）"><a href="#集成模型（Ensemble）" class="headerlink" title="集成模型（Ensemble）"></a>集成模型（Ensemble）</h3><p><strong>模型介绍：</strong></p>
<ul>
<li><p>综合考量多个分类器的预测结果，从而做出分类决策，<strong>综合考量</strong>方式有2种：</p>
</li>
<li><p>1.<strong>利用相同的训练数据同时搭建多个独立的分类模型</strong>，然后通过<strong>投票</strong>以少数服从多数的原则做出最终分类决策，如：</p>
</li>
<li><p><strong>随机森林分类器</strong>(Random Forest Classifier)：在相同训练数据上同时搭建多棵决策树（每棵树都随机选取特征）</p>
</li>
<li><p>2.<strong>按照一定词序搭建多个分类模型，模型间彼此存在依赖关系</strong>（每个后续模型的加入都需要对现有集成模型的综合性能有所贡献，进而不断提升更新过后的集成模型的性能，并最终期望借助整合多个分类能力较弱的分类器，搭建出具有更强分类能力的模型），如:</p>
</li>
<li><p><strong>梯度提升决策树</strong>(Gradient Tree Boosting)：每棵树在生成过程中都会尽可能降低整体集成模型在训练集上的拟合误差</p>
</li>
</ul>
<p><strong>特点分析：</strong></p>
<ul>
<li><p>最常见的应用；可整合多种模型</p>
</li>
<li><p>缺点：模型估计参数的过程受概率影响，具有不确定性</p>
</li>
<li><p>优点：虽然模型训练需要耗费更多时间，但得到的综合模型会具有<strong>更高的性能和稳定性</strong></p>
</li>
</ul>
<h2 id="回归预测"><a href="#回归预测" class="headerlink" title="回归预测"></a><strong>回归预测</strong></h2><h3 id="线性回归器"><a href="#线性回归器" class="headerlink" title="线性回归器"></a>线性回归器</h3><p><strong>模型介绍：</strong></p>
<blockquote>
<p><strong>最小二乘法</strong>：数学优化方法；通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。</p>
</blockquote>
<p>线性回归问题中：优化目标即最小化预测结果与真实值之间的差异（因为预测目标直接是实数域上的数值）</p>
<p>当使用一组m个用于训练的特征向量$X=$ 和其对应的回归目标$y=$ 时，我们希望线性回归模型可以<strong>最小二乘</strong>(Generalized Least Squares)<strong>预测的损失$L(w,b)$</strong> ，则<strong>线性回归器的常见优化目标为</strong>：*</p>
<p>同样为学习到决定模型的参数$w，b$ ，仍可使用一种精确计算的解析算法和一种快速的随机梯度下降(Stochastic Gradient Descend)估计算法.</p>
<p><strong>性能评估指标：</strong></p>
<p>假设测试数据有m个目标数值$y= $ 且记$\overline{y}$ 为回归模型的预测结果，则：</p>
<ul>
<li><p><strong>MAE（平均绝对误差）</strong>：</p>
</li>
<li><p>$SS<em>{abs}=\sum</em>{m}^{i=1}\qquad|y^i-\overline{y}|$ ,$</p>
</li>
<li><p>$MAE=\frac{SS_{abs}}{m}$</p>
</li>
<li><p><strong>MSE（均方误差）</strong>：</p>
</li>
<li><p>$SS<em>{tot}=\sum</em>{m}^{i=1}\qquad(y^i-\overline{y})^2$</p>
</li>
<li><p>Missing close brace MSE=\frac{SS_{tot}{m}</p>
</li>
<li><p><strong>R-squared（R平方）</strong>：</p>
</li>
<li><p>$SS<em>{res}=\sum</em>{m}^{i=1}\qquad(y^i-(f(x^i))^2$</p>
</li>
<li><p>Missing close braceR^2=1-\frac{SS_{res}{tot}</p>
</li>
<li><p>其中，$SS<em>{tot}$ 代表测试数据真实值的方差(内部差异)；$SS</em>{res}$ 代表回归值与真实值之阿金的平方差异（回归差异）</p>
</li>
<li><p><strong>R-squared（拟合度）</strong>：<strong>比较预测结果与真实值的吻合程度</strong>，既考量了回归值与真实值的差异，又兼顾了问题本身真实值的变动；而MAE、MSE(差值的绝对值或平方)则会随不同预测问题而变化巨大，欠缺在不同问题中的可比性</p>
</li>
</ul>
<p><strong>使用三种回归评价机制和两种调用R-squared评价模块的方法，评价此模型的回归性能</strong></p>
<table>
<thead>
<tr>
<th>1234567891011121314151617181920212223242526</th>
<th># 使用LR模型自带的评估模块print ‘The value of default measurement of LR is’,lr.score(X_test,y_test) # 导入MAE和MSE评估回归模型from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error # 使用r2_score模块print ‘The value of R-squared of LR is’,r2_score(y_test,lr_y_predict) # 使用mean_squared_error模块print ‘The MSE of LR is’,mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict)) # 使用mean_absolute_error模块print ‘The MAE of LR is’,mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(lr_y_predict)) # 使用SGDR自带评估模块print ‘The value of default measurement of SGDR is’,sgdr.score(X_test,y_test) # 使用r2_score模块print ‘The value of R-squared of SGDR is’,r2_score(y_test,sgdr_y_predict) # 使用mean_squared_error模块print ‘The MSE of SGDR is’,mean_squared_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(sgdr_y_predict)) # 使用mean_absolute_error模块print ‘The MAE of SGDR is’,mean_absolute_error(ss_y.inverse_transform(y_test),ss_y.inverse_transform(sgdr_y_predict))</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>特点分析：</strong></p>
<ul>
<li><p><strong>数据规模超10万，使用随机梯度法估计参数</strong></p>
</li>
<li><p>在不清楚特征之间关系的前提下，可使用线性回归模型作为基线系统（baseline system）</p>
</li>
</ul>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><p><strong>模型介绍：</strong></p>
<ul>
<li>同样是<strong>从训练数据中选取一部分更加有效的支持向量</strong>，只是这少部分训练样本所提供的并不是类别目标，而是<strong>具体的预测数值</strong></li>
</ul>
<p>继续使用2.1.3.1中的训练集和测试集进行不同核函数配置的SVM回归模型训练，且分别对测试数据做出越策，会发现：</p>
<ul>
<li><p>不同配置下的模型在相同测试集上存在非常大的性能差异，且使用径向基(Radical basis function)核函数对特征进行非线性映射后，SVM展现最佳回归性能</p>
</li>
<li><p>可以多尝试几种配置，以活动最佳预测性能</p>
</li>
</ul>
<blockquote>
<p>核函数：一种特征映射技巧，即通过某种函数计算，将原有的线性不可分的低维特征映射到更高维度的空间，从而尽可能达到新的高维度特征线性可分的程度。</p>
</blockquote>
<h3 id="K近邻"><a href="#K近邻" class="headerlink" title="K近邻"></a>K近邻</h3><p><strong>模型介绍：</strong></p>
<ul>
<li>在回归任务中，K近邻(回归)模型同样<strong>只是借助周围K个距离最近的训练样本的目标数值</strong>，对待测样本的回归值进行决策。</li>
</ul>
<p><strong>1.使用2种不同配置的K近邻回归模型对美国波士顿放假数据进行回归预测</strong></p>
<table>
<thead>
<tr>
<th>1234567891011121314151617181920</th>
<th>from sklearn.datasets import load_bostonboston=load_boston() from sklearn.cross_validation import train_test_splitimport numpy as npX=boston.datay=boston.targetX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33) from sklearn.neighbors import KNeighborsRegressor # 初始化回归器，调整配置，使预测方式为平均回归，weights=’uniform’uni_knr=KNeighborsRegressor(weights=’uniform’)uni_knr.fit(X_train,y_train)uni_knr_y_predict=uni_knr.predict(X_test) # 初始化回归器，调整配置，使预测方式为根据距离加权回归，weights=’distance’dis_knr=KNeighborsRegressor(weights=’distance’)dis_knr.fit(X_train,y_train)dis_knr_y_predict=dis_knr.predict(X_test)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>由上可知，<strong>K近邻加权平均的回归策略具有更好的预测性能</strong>。</p>
<h3 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><p><strong>模型介绍：</strong></p>
<ul>
<li><p>在<strong>选择不同特征作为分裂节点的策略</strong>上，与决策树类似</p>
</li>
<li><p>不同：<strong>回归树叶节点的数据类型为连续型非离散型</strong>；决策树每个叶子节点依照训练数据表现的概率倾向决定其最终的预测类别，而回归树叶子节点是一个个具体数值，从预测值连续的意义上严格讲，回归树不能称为回归算法（因为<strong>回归树叶子节点返回的是“一团”训练数值的均值，而非具体连续的预测值</strong>）</p>
</li>
</ul>
<p><strong>树模型（回归树，决策树）</strong></p>
<ul>
<li><p>优点：可<strong>解决非线性拟合问题</strong>；<strong>不要求对特征标准化和统一量化</strong>（即数值型、类别型特征都可直接被训练）；可直观输出决策过程，使决策结果具有<strong>可解释性</strong></p>
</li>
<li><p>缺点：容易因为模型搭建得过于复杂而<strong>丧失对新数据的精确预测能力（泛化能力）</strong>；树模型从上至下的预测流程会因为数据细微的更改而发生较大的结构变化，故预测<strong>稳定性较差</strong>；在有限时间内无法找到最优解（而只是次优解）</p>
</li>
</ul>
<h3 id="集成模型"><a href="#集成模型" class="headerlink" title="集成模型"></a>集成模型</h3><p>补充：极端随机森林(Extremely Randomized Trees)</p>
<ul>
<li>每构建一棵树的分裂节点时，不会任意选取特征，而是先随机选取一部分特征，然后利用信息熵(Information Gain)和基尼不纯性(Gini Impurity)等指标挑选出最佳节点特征</li>
</ul>
<p><strong>1.使用三种集成回归模型对波士顿房间训练数据进行学习，并对测试数据进行预测</strong></p>
<table>
<thead>
<tr>
<th>1234567891011121314151617181920212223</th>
<th>from sklearn.datasets import load_bostonboston=load_boston() from sklearn.cross_validation import train_test_splitimport numpy as npX=boston.datay=boston.targetX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33)  from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor rfr=RandomForestRegressor()rfr=rfr.fit(X_train,y_train)rfr_y_predict=rfr.predict(X_test) etr=ExtraTreesRegressor()etr=etr.fit(X_train,y_train)etr_y_predict=etr.predict(X_test) gbr=GradientBoostingRegressor()gbr=gbr.fit(X_train,y_train)gbr_y_predict=gbr.predict(X_test)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="无监督学习经典模型"><a href="#无监督学习经典模型" class="headerlink" title="无监督学习经典模型"></a>无监督学习经典模型</h2><h2 id="数据聚类"><a href="#数据聚类" class="headerlink" title="数据聚类"></a>数据聚类</h2><h3 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h3><p>模型介绍</p>
<ul>
<li><p>最经典易用的聚类模型；要求预先设定聚类个数，然后不断更新聚类中心，经过几轮迭代，最后的目标是让<strong>所有数据点到其所属聚类中心距离的平方和趋于稳定</strong></p>
</li>
<li><p>算法执行的过程分4个阶段：</p>
</li>
<li><p>1.随机布设K个特征空间内的点作为初始的聚类中心；</p>
</li>
<li><p>2.根据每个数据的特征向量，从K个聚类中心中寻找距离最近的一个，并且把该数据标记为从属于这个聚类中心；</p>
</li>
<li><p>3.在所有数据都被标记过聚类中心之后，根据这些数据新分配的类簇，重新对K个聚类中心做计算；</p>
</li>
<li><p>4.若一轮下来，所有数据点从属的聚类中心与上一次分配的类簇没有变化，则迭代可停止，否则回到步骤2继续循环</p>
</li>
</ul>
<p>​         <img src="6.jpg" alt="img">       </p>
<p><strong>聚类算法的性能评估指标</strong>：</p>
<p>1.若被评估数据已被标注正确的类别，则使用2个指标：</p>
<ul>
<li><p><strong>ARI指标</strong>（Adjusted Rand Index）</p>
</li>
<li><p><strong>Accuracy</strong>（准确性，同分类问题）</p>
</li>
</ul>
<p>2.若被评估数据无所属类别，则使用<strong>轮廓系数（Silhouette Coefficient）</strong>来度量聚类结果的质量，说明：</p>
<ul>
<li><p>轮廓系数<strong>兼顾聚类的凝聚度（Cohesion）和分离度（Separation）</strong></p>
</li>
<li><p>取值范围：[-1,1]，<strong>轮廓系数值越大，则聚类效果越好</strong></p>
</li>
<li><p>具体计算步骤：</p>
</li>
<li><p>1.对已聚类数据中第$i$个样本$x^i$ ,计算$x^i$与其同一个类簇内的所有其他样本距离的平均值，记作$a^i$ ,用于量化簇内的凝聚度；</p>
</li>
<li><p>2.选取$x^i$ 外的一个簇$b$，计算$x^i$与簇$b$中所有样本的平均距离，遍历所有其他簇，找到最近的这个平均距离，记作$b^i$ ，用于量化簇之间分离度；</p>
</li>
<li><p>3.对于样本$x^i$ ，轮廓系数为$sc^i=\frac{b^i-a^i}{max(b^i,a^i)}$ ;</p>
</li>
<li><p>4.最后对所有样本$X$求出平均值，即为当前聚类结果的整体轮廓系数</p>
</li>
<li><p>衡量效果：</p>
</li>
<li><p>若$sc^i &lt; 0$ ,则说明$x^i$ 与其簇内元素的平均距离大于最近的其他簇，表示聚类效果不好；</p>
</li>
<li><p>若$a^i$ 趋于0，或$b^i$ 足够大，则$sc^i $ 趋于1 ,表示聚类效果好；</p>
</li>
</ul>
<p>特点分析：</p>
<ul>
<li><p>K-means聚类模型采取的是<strong>迭代式算法</strong></p>
</li>
<li><p><strong>缺点：容易收敛到局部最优解；需要预先设定簇的数量</strong></p>
</li>
</ul>
<p><strong>局部最优解</strong>：</p>
<ul>
<li><p>最优化：在复杂环境中遇到的许多可能的决策中，挑选“最好”的决策</p>
</li>
<li><p>局部最优：指对于一个问题的解在一定范围或区域内最优，或者说解决问题或达成目标的手段在一定范围或限制内最优（和全局最优不同，局部最优不要求在所有决策中是最好的）</p>
</li>
<li><p>全局最优：针对一定条件/环境下的一个问题/目标，若一项决策和<strong>所有</strong>解决该问题的决策相比是最优的，就可以被称为全局最优</p>
</li>
<li><p>如下图：左边是实际数据和正确的所属类簇；右下的局部最优情况导致无法继续更新聚类中心，使聚类结果与正确结果相差很大</p>
</li>
<li><p>“容易收敛到局部最优解”是算法自身的缺陷，但<strong>可通过执行多次kmeans算法来挑选性能最好的初始中心点</strong></p>
</li>
</ul>
<p>​         <img src="7.jpg" alt="img">       </p>
<p><strong>肘部观察法</strong>：</p>
<p>作用：<strong>粗略估计相对合理的类簇个数</strong></p>
<ul>
<li>思路：因为K-means模型最终期望所有数据点到其所属的类簇举例的平方和趋于稳定，所以我们可以通过观察这个数值随K的走势来找出最佳的类簇数量；理想条件下，这个折线在不断下降且趋于平缓的过程中会有斜率的拐点，即从这个拐点对应的K值开始，类簇中心的增加不会过于破坏数据聚类的结构（<strong>进一步增加K值不会再有利于算法的收敛</strong>），则<strong>此拐点K=n是相对最佳的类簇数量</strong>。</li>
</ul>
<p>​         <img src="8.jpg" alt="img">       </p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="9.jpg" alt="img"></h1><p><strong>肘部观察法示例</strong>：</p>
<table>
<thead>
<tr>
<th>1234567891011121314151617181920212223242526272829303132</th>
<th>import numpy as npfrom sklearn.cluster import KMeansfrom scipy.spatial.distance import cdistimport matplotlib.pyplot as plt # 使用均匀分布函数随机三个簇，每个簇周围10个数据样本cluster1=np.random.uniform(0.5,1.5,(2,10))cluster2=np.random.uniform(5.5,6.5,(2,10))cluster3=np.random.uniform(10.5,11.5,(2,10)) # 绘制30个数据样本的分布图像X=np.hstack((cluster1,cluster2,cluster3)).Tplt.scatter(X[:,0],X[:,1])plt.xlabel(‘x1’)plt.ylabel(‘x2’)plt.show() # 测试9种不同聚类中心数量下，每种情况的聚类质量K=range(1,10)meandistortions=[] for k in K:    kmeans=KMeans(n_clusters=k)    kmeans.fit()    meandistortions.append(sum(np.min(cdist(X,kmeans.cluster_centers_,’euclidean’),axis=1))/X.shape[0])     plt.plot(K,meandistortions,’bx-‘)plt.xlabel(‘k’)plt.ylabel(‘Average Dispersion’)plt.title(‘Selecting k with the Elbow Method’)plt.show()</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h2><ul>
<li><p>特征维度过高，无法构建有效特征；无法肉眼观测超过三个维度的特征</p>
</li>
<li><p>重构有效的低维特征向量，为数据拓展提供可能</p>
</li>
</ul>
<h3 id="主成分分析（Principle-Component-Analysis）"><a href="#主成分分析（Principle-Component-Analysis）" class="headerlink" title="主成分分析（Principle Component Analysis）"></a>主成分分析（Principle Component Analysis）</h3><p>模型介绍</p>
<ul>
<li><p><strong>最经典使用的特征降维技术；辅助图像识别</strong></p>
</li>
<li><p>举例：若我们有一组2*2的数据[(1,2),(2,4)]，假设这两个数据都反映到一个类别或类簇；若我们的学习模型是线性模型，则这两个模型只能帮助权重参数更新1次，因为他们线性相关，所有特征值只是扩张了相同背书；若使用PCA分析，则此矩阵的“秩”=1，即在多样性程度上，此矩阵只有1个自由度。</p>
</li>
<li><p>可把PCA当做特征选择，<strong>这种特征选择是先把原来的特征空间作了映射，使得新的映射后特征空间数据彼此正交；则我们通过主成分分析就尽可能保留下具备区分性的低维数据特征</strong>。</p>
</li>
</ul>
<blockquote>
<p>矩阵的秩：一个矩阵A的列秩是A的<strong>线性独立的纵列的极大数目</strong>，通常表示为r(<em>A</em>)或rank <em>A</em>。</p>
</blockquote>
<blockquote>
<p>自由度：统计学上，指当以样本的统计量来估计总体的参数时，<strong>样本中独立或能自由变化的数据的个数</strong>；数学上，<strong>自由度是一个随机向量的维度数，即一个向量能被完整描述所需的最少单位向量数</strong>。如从电脑屏幕到厨房的位移能够用三维向量$\widehat{ai}+\widehat{bj}+\widehat{ck}$来描述，因此这个位移向量的自由度是3。自由度也通常与这些向量的座标平方和，以及卡方分布中的参数有所关联。</p>
</blockquote>
<p><strong>求线性相关矩阵的秩</strong>：</p>
<table>
<thead>
<tr>
<th>123</th>
<th>import numpy as np test = np.array([[1,2],[2,4]])print np.linalg.matrix_rank(test,tol=None)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>1.显示手写体数字图片经PCA压缩后的二维空间分布</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 分割训练数据的特征向量和标记，前64维是feature vector，第65维是标记</span><br><span class="line">X_digits&#x3D;digits_train[np.arange(64)]</span><br><span class="line">y_digits&#x3D;digits_train[64]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 导入PCA</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"># 初始化一个可将高维向量压缩到二维的PCA</span><br><span class="line">estimator&#x3D;PCA(n_components&#x3D;2)</span><br><span class="line">X_pca&#x3D;estimator.fit_transform(X_digits)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 显示10类图像经PCA压缩后的二维空间分布</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_pca_scatter():</span><br><span class="line">    </span><br><span class="line">    colors&#x3D;[&#39;black&#39;,&#39;blue&#39;,&#39;purple&#39;,&#39;yellow&#39;,&#39;white&#39;,&#39;red&#39;,&#39;lime&#39;,&#39;cyan&#39;,&#39;orange&#39;,&#39;gray&#39;]</span><br><span class="line">    </span><br><span class="line">    for i in xrange(len(colors)):</span><br><span class="line">        px&#x3D;X_pca[:,0][y_digits.as_matrix()&#x3D;&#x3D;i]</span><br><span class="line">        py&#x3D;X_pca[:,1][y_digits.as_matrix()&#x3D;&#x3D;i]</span><br><span class="line">        plt.scatter(px,py,c&#x3D;colors[i])</span><br><span class="line">    </span><br><span class="line">    plt.legend(np.arange(0,10).astype(str))</span><br><span class="line">    plt.xlabel(&#39;First Principle Component&#39;)</span><br><span class="line">    plt.ylabel(&#39;Second Principle Component&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line">    plt_pca_scatter</span><br></pre></td></tr></table></figure>

<p><strong>2.使用原始像素特征和经PCA压缩重建的低维特征，在相同配置的SVM上分别进行图像识别</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">X_train&#x3D;digits_train[np.arange(64)]</span><br><span class="line">y_train&#x3D;digits_train[64]</span><br><span class="line">X_test&#x3D;digits_test[np.arange(64)]</span><br><span class="line">y_test&#x3D;digits_test[64]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 导入基于线性核的SVM分类器,建模，预测</span><br><span class="line">from sklearn.svm import LinearSVC</span><br><span class="line">svc&#x3D;LinearSVC()</span><br><span class="line">svc.fit(X_train,y_train)</span><br><span class="line">y_predict&#x3D;svc.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 特征压缩到20维,并转化原训练特征</span><br><span class="line">estimator&#x3D;PCA(n_components&#x3D;20)</span><br><span class="line">pca_X_train&#x3D;estimator.fit_transform(X_train)</span><br><span class="line">pca_X_test&#x3D;estimator.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 对压缩后的20维特征的训练数据进行建模，并对测试集预测</span><br><span class="line">pca_svc&#x3D;LinearSVC()</span><br><span class="line">pca_svc.fit(pca_X_train,y_train)</span><br><span class="line">pca_y_predict&#x3D;pca_svc.predict(pca_X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print pca_y_predict,y_pre</span><br></pre></td></tr></table></figure>

<p><strong>3.性能评估</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print svc.score(X_test,y_test)</span><br><span class="line">print classification_report(y_test,y_predict,target_names&#x3D;np.arange(10).astype(str))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print pca_svc.score(pca_X_test,y_test)</span><br><span class="line">print classification_report(y_test,pca_y_predict,target_names&#x3D;np.arange(10).astype(str)</span><br></pre></td></tr></table></figure>

<p>由上发现，经过PCA处理后会损失2%左右的预测准确性，但相比原始数据64维度的特征，使用PCA可降低68.75%的维度、</p>
<p><strong>特点分析：</strong></p>
<ul>
<li><p><strong>降维/压缩是选取数据具有代表性的特征，在保持数据多样性(Variance)的基础上，规避掉大量的特征冗余和噪声；并可节省模型训练时间，提高综合效率</strong></p>
</li>
<li><p>但<strong>容易损失一些有用的模式信息</strong></p>
</li>
</ul>
<h1 id="第三章-进阶篇"><a href="#第三章-进阶篇" class="headerlink" title="第三章 进阶篇"></a>第三章 进阶篇</h1><p>前一节使用的数据集都是经过规范化处理的的规整数据集，使用的模型也都是默认配置，但现实生活中我们得到的数据集不会如此规整，默认配置也不一定最佳。</p>
<p>本章目的：掌握如何通过<strong>抽取或筛选数据特征、优化模型配置</strong>，以进一步提升经典模型的性能表现。</p>
<h2 id="模型实用技巧"><a href="#模型实用技巧" class="headerlink" title="模型实用技巧"></a>模型实用技巧</h2><p>依靠默认配置学习到模型所需的参数，不能保证：</p>
<ul>
<li><p>所有用于训练的数据特征都是最好的</p>
</li>
<li><p>学习到的参数一定是最优的</p>
</li>
<li><p>默认配置下的模型总是最佳的</p>
</li>
</ul>
<p>本节技巧：<strong>预处理数据，控制参数训练、优化模型配置</strong>,etc</p>
<h3 id="特征提升"><a href="#特征提升" class="headerlink" title="特征提升"></a>特征提升</h3><p><strong>特征抽取</strong>：<strong>逐条将原始数据转化为特征向量</strong>的形式，这个过程同时涉及到<strong>对数据特征的量化表示</strong>；</p>
<p><strong>特征筛选</strong>：(更进一步)<strong>在高维度、已量化的特征向量中选择对指定任务更有效的特征组合</strong>，进一步提升模型性能</p>
<p>3.1.1.1 特征抽取</p>
<p>原始数据的种类有很多：数字化的信号数据(声纹、图像)，符号化的文本；而我们无法直接将符号化的文本用于计算，而需要通过某些处理手段预先将文本良华为特征向量。</p>
<p><strong>1.DictVectorizer对使用字典存储的数据进行特征抽取和向量化</strong></p>
<blockquote>
<p>有些符号化的数据特征已相对结构化，并以字典这种数据结构进行存储，故可使用DictVectorizer对特征进行抽取和向量化。</p>
</blockquote>
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>123456789101112</th>
<th>M=[{‘city’:’Dubai’,’temperature’:33.},{‘city’:’London’,’temperature’:12.},{‘city’:’Beijing’,’temperature’:40.}] from sklearn.feature_extraction import DictVectorizer # 初始化特征抽取器vec=DictVectorizer() # 输出转化后的特征矩阵print vec.fit_transform(M).toarray() # 输出各维度特征的含义print vec.get_feature_names()</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>由输出可知，特征向量化过程中。DictVectorizer对类别型和数值型特征的处理方式不同。</p>
<ul>
<li><p>类别型(categorical)特征：借助原特征名称组合产生新特征，并用0/1二值方式进行量化</p>
</li>
<li><p>数值型(numerical)：维持原始特征值</p>
</li>
</ul>
<p><strong>2.使用CountVectorizer且在不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试</strong></p>
<p>处理文本数据的方法：<strong>词袋法(Bag of Words)</strong></p>
<ul>
<li><p><strong>词袋法</strong>：不考虑词语出现的顺序，只将训练文本中的每个出现过的词汇单独视作一列特征；词表：不重复的词汇的集合；每条训练文本都可在高维度词表上映射出一个特征向量；</p>
</li>
<li><p>特征数值的常见计算方式：CountVectorizer &amp; TfidfVectorizer</p>
</li>
<li><p><strong>CountVectorizer</strong>：只考虑每种词汇(Term)在该条训练文本中出现的频率(Term Frequency)</p>
</li>
<li><p><strong>TfidfVectorizer</strong>：既考量某一次会在当前文本中出现的频率，又考虑包含这个词汇的文本条数的倒数(Inverse Document Frequency),即训练的条目越多，TfidfVectorizer的特征量化就越有优势；可剔除在每条文本中都出现的常用词汇，以减少它们对模型分类决策的影响</p>
</li>
<li><p><strong>停用词</strong>(Stop Words)：在每条文本中都出现的常用词汇，如the,a；停用词常在特征抽取中以黑名单的方式过滤掉，以提高模型的性能表现</p>
</li>
<li><p>区别：<strong>CountVectorizer只统计词频，而TfidfVectorizer还过滤掉了停用词</strong></p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line">news&#x3D;fetch_20newsgroups(subset&#x3D;&#39;all&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train,y_train,X_test,y_test&#x3D;train_test_split(news.data,news.target,test_size&#x3D;0.25,random_state&#x3D;33)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">count_vec&#x3D;CountVectorizer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 只使用词频统计将原始训练和测试文本转化为特征向量</span><br><span class="line">X_count_train&#x3D;count_vec.fit_transform(X_train)</span><br><span class="line">X_count_test&#x3D;count_vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 导入naive bayes,默认配置初始化，使用CountVectorizer(未剔除停用词的)后的训练样本进行学习</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">mnb_count&#x3D;MultinomialNB()</span><br><span class="line">mnb_count.fit(X_count_train,y_train)</span><br><span class="line">y_count_predict&#x3D;mnb_count.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 输出性能评估结果</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line">print &#39;Accuracy:&#39;,mnb_count.score(X_count_train,y_train)</span><br><span class="line">print classification_report(y_test,y_count_predict,target_names&#x3D;news.target_na</span><br></pre></td></tr></table></figure>

<p><strong>3.使用TfidfVectorizer且在不去掉停用词的条件下，对文本特征进行量化的朴素贝叶斯分类性能测试</strong></p>
<table>
<thead>
<tr>
<th>12345678910</th>
<th>from sklearn.datasets import fetch_20newsgroups # 即时从网上下载数据news=fetch_20newsgroups(subset=’all’) print len(news.data)print news.data[0] from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(news.data,news.target,test_size=0.25,random_state=33)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>12345678910111213141516</th>
<th>from sklearn.feature_extraction.text import TfidfVectorizertfidf_vec=TfidfVectorizer() # 转化为特征向量X_tfidf_train=tfidf_vec.fit_transform(X_train)X_tfidf_test=tfidf_vec.transform(X_test) from sklearn.naive_bayes import MultinomialNBmnb_tfidf=MultinomialNB()mnb_tfidf.fit(X_tfidf_train,y_train)y_tfidf_predict=mnb_tfidf.predict(X_tfidf_test) # 性能评估print ‘Accuracy:’,mnb_tfidf.score(X_tfidf_test,y_test)from sklearn.metrics import classification_reportprint classification_report(y_test,y_tfidf_predict,target_names=news.target_names)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>由输出可知，在使用TfidfVectorizer而不去掉停用词的条件下，对训练和测试文本进行特征量化，并利用默认配置的naive bayes，在测试文本上可得到比CountVectorizer更高的预测准确性。证明：<strong>在训练文本量较多时，使用TfidfVectorizer压制常用词汇对分类决策的干扰，可提升模型性能</strong>。</p>
<p><strong>4.分别使用CountVectorizer和TfidfVectorizer，并在去掉停用词的条件下，对文本特征进行量化的Naive Bayes分类性能测试</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># 分别使用停用词过滤器配置初始化CountVectorizer和TfidfVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">count_filter_vec,tfidf_filter_vec&#x3D;CountVectorizer(analyzer&#x3D;&#39;word&#39;,stop_words&#x3D;&#39;english&#39;),TfidfVectorizer(analyzer&#x3D;&#39;word&#39;,stop_words&#x3D;&#39;english&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用带停用词过滤的CountVectorizer对训练和测试文本进行量化处理</span><br><span class="line">X_count_filter_train&#x3D;count_filter_vec.fit_transform(X_train)</span><br><span class="line">X_count_filter_test&#x3D;count_filter_vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用带停用词过滤的TfidfVectorizer对训练和测试文本进行量化处理</span><br><span class="line">X_tfidf_filter_train&#x3D;tfidf_filter_vec.fit_transform(X_train)</span><br><span class="line">X_tfidf_filter_test&#x3D;tfidf_filter_vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 初始化默认配置的朴素贝叶斯，并对CountVectorizer后的数据进行预测和性能评估</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">mnb_count_filter&#x3D;MultinomialNB()</span><br><span class="line">mnb_count_filter.fit(X_count_filter_train,y_train)</span><br><span class="line">y_count_predict&#x3D;mnb_count_filter.predict(X_count_filter_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 初始化另一个默认配置的朴素贝叶斯，并对TfidfVectorizer后的数据进行预测和性能评估</span><br><span class="line">mnb_tfidf_filter&#x3D;MultinomialNB()</span><br><span class="line">mnb_tfidf_filter.fit(X_tfidf_filter_train,y_train)</span><br><span class="line">y_tfidf_predict&#x3D;mnb_tfidf_filter.predict(X_tfidf_filter_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># CountVectorizer性能评估</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line">print &#39;Count_Accuracy&#39;,mnb_count_filter.score(X_count_filter_train,y_train)</span><br><span class="line">print classification_report(y_test,y_count_predict,target_names&#x3D;news.target_names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TfidfVectorizer性能评估</span><br><span class="line">print &#39;Tfidf_Accuracy&#39;,mnb_tfidf_filter.score(X_tfidf_filter_train,y_train)</span><br><span class="line">print classification_report(y_test,y_tfidf_predict,target_names&#x3D;news.target_</span><br></pre></td></tr></table></figure>

<p>由输出可知，<strong>TfidfVectorizer的特征抽取和量化方法更具备优势，对停用词进行过滤后的模型性能比未过滤高3%—4%</strong>。</p>
<p>3.1.1.2 特征筛选</p>
<p>良好的数据特征组合可提高模型性能，冗余特征会浪费CPU计算资源，不良特征会降低模型精度。</p>
<p>主成分分析(PCA)：用于去除线性相关的特征组合</p>
<p>特征筛选：不是修改特征值，而是寻找对模型性能提升大的少量特征</p>
<p><strong>使用Titanic数据集，通过特征筛选法一步步提升决策树的预测性能</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"># 导入数据</span><br><span class="line">import pandas as pd</span><br><span class="line">titanic&#x3D;pd.read_csv(&#39;&#x2F;Users&#x2F;scarlett&#x2F;repository&#x2F;projects&#x2F;titanic&#x2F;titanic.csv&#39;)</span><br><span class="line">print titanic.shape</span><br><span class="line">print titanic.info()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分离数据特征与预测目标</span><br><span class="line">y&#x3D;titanic[&#39;survived&#39;]</span><br><span class="line">X&#x3D;titanic.drop([&#39;row.names&#39;,&#39;name&#39;,&#39;survived&#39;],axis&#x3D;1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 填充缺失数据</span><br><span class="line">X[&#39;age&#39;].fillna(X[&#39;age&#39;].mean(),inplace&#x3D;True)</span><br><span class="line">X.fillna(&#39;UNKNOWN&#39;,inplace&#x3D;True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 分割数据</span><br><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train,y_train,X_test,y_test&#x3D;train_test_split(X,y,test_size&#x3D;0.25,random_state&#x3D;33)</span><br><span class="line"># 类别型特征向量化</span><br><span class="line">from sklearn.feature_extraction import DictVectorizer</span><br><span class="line">vec&#x3D;DictVectorizer()</span><br><span class="line">X_train&#x3D;vec.fit_transform(X_train.to_dict(orient&#x3D;&#39;record&#39;))</span><br><span class="line">X_test&#x3D;vec.transform(X_test.to_dict(orient&#x3D;&#39;record&#39;))</span><br><span class="line">print len(vec.feature_names_)</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">dt&#x3D;DecisionTreeClassifier()</span><br><span class="line">dt.fit(X_train,y_train)</span><br><span class="line">dt.score(X_test,y_test)</span><br><span class="line"># 导入特征筛选器</span><br><span class="line">from sklearn import feature_selection</span><br><span class="line">fs&#x3D;feature_selection.SelectPercentile(feature_selection.chi2,percentile&#x3D;20)</span><br><span class="line">X_train_fs&#x3D;fs.fit_transform(X_train,y_train)</span><br><span class="line">dt.fit(X_train_fs,y_train)</span><br><span class="line">X_test_fs&#x3D;fs.transform(X_test)</span><br><span class="line">dt.score(X_test_fs,y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 通过交叉验证法，按照固定间隔的百分比筛选特征，并作图展示性能岁特征筛选比例的变化</span><br><span class="line">from sklearn.cross_validation import cross_val_score</span><br><span class="line">import numpy as np</span><br><span class="line">percentile&#x3D;range(1,100,2)</span><br><span class="line">results&#x3D;[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for i in percentile:</span><br><span class="line">    fs&#x3D;feature_selection.SelectPercentile(feature_selection.chi2,percentile&#x3D;i)</span><br><span class="line">    X_train_fs&#x3D;fs.fit_transform(X_train,y_train)</span><br><span class="line">    scores&#x3D;cross_val_score(dt,X_train_fs,y_train,cv&#x3D;5)</span><br><span class="line">    results&#x3D;np.append(results,scores.mean())</span><br><span class="line">    print results</span><br><span class="line">    </span><br><span class="line"># 找到提现最佳性能的特征筛选的百分比</span><br><span class="line">opt&#x3D;np.where(results&#x3D;&#x3D;results.max())[0]</span><br><span class="line">print &#39;Optimal number of features %d&#39;%percentiles[o</span><br></pre></td></tr></table></figure>

<h3 id="模型正则化"><a href="#模型正则化" class="headerlink" title="模型正则化"></a>模型正则化</h3><p>任何机器学习模型在训练集上的性能表现都不能作为其对未知测试数据预测能力的评估。</p>
<p>本节：<strong>模型泛化力</strong>(Generalization)，和如何保证模型泛化力</p>
<p>3.1.2.1 欠拟合和过拟合(Underfitting &amp; Overfitting)</p>
<p><strong>拟合</strong>：机器学习模型在训练过程中，<strong>通过更新参数，使模型不断契合可观测数据(训练集)的过程</strong>。</p>
<p>阐述：模型复杂度与泛化力的关系</p>
<p><strong>1.使用线性回归模型在披萨训练样本上进行拟合</strong></p>
<table>
<thead>
<tr>
<th>123456789101112131415161718192021222324252627</th>
<th>X_train=[[6],[8],[10],[14],[18]]y_train=[[7],[9],[13],[17.5],[18]] # 导入线性回归模型from sklearn.linear_model import LinearRegressionregressor=LinearRegression()regressor.fit(X_train,y_train) # 导入numpyimport numpy as np# 在x轴上从0-25均匀采样100个数据点,并以100个数据点为基准，预测回归直线xx=np.linspace(0,26,100)xx=xx.reshape(xx.shape[0],1)yy=regressor.predict(xx) # 对预测到的直线作图,import matplotlib.pyplot as pltplt.scatter(X_train,y_train)# 使用plt.plot()画(x,y)曲线,degree=1表示特征是一维的，做个标记plt1,=plt.plot(xx,yy,label=”Degree=1”)plt.axis([0,25,0,25]) # axis表示坐标的极值范围plt.xlabel(‘Diameter of pizza’)plt.ylabel(‘Price’) plt.show() # 输出模型在训练样本上的R-squared值print regressor.score(X_train,y_train)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>特征多项式次数</th>
<th>训练集R-squared值</th>
<th>测试集R-squared值</th>
</tr>
</thead>
<tbody><tr>
<td>degree=1</td>
<td>0.9100</td>
<td>0.8097</td>
</tr>
<tr>
<td>degree=2</td>
<td>0.9816</td>
<td>0.8675</td>
</tr>
<tr>
<td>degree=4</td>
<td>1.0000</td>
<td>0.8096</td>
</tr>
</tbody></table>
<p>由输出可见</p>
<ul>
<li><p><strong>欠拟合</strong>：当模型复杂度很低时(degree=1)，模型既在训练集上拟合不好，又在测试集上表现一般</p>
</li>
<li><p><strong>过拟合</strong>：一味追求高模型复杂度(degree=4)，尽管模型完美拟合了几乎所有训练数据，但模型会变得非常波动，几乎<strong>丧失了对未知数据的预测能力</strong></p>
</li>
</ul>
<p>这两种都是<strong>模型缺乏泛化力</strong>的表现。</p>
<p>要求我们<strong>在增加模型复杂度、提高在可观测数据上的性能表现的同时，需要兼顾模型的泛化力，防止发生过拟合</strong>。为了平衡这两种选择，我们通常<strong>采用2种模型正则化方法：L1范数正则化 &amp; L2范数正则化</strong></p>
<p>3.1.2.2 L1范数正则化</p>
<p><strong>正则化(Regularization)</strong></p>
<ul>
<li><p>目的：<strong>提高模型在位置测试数据上的泛化力，避免过拟合</strong></p>
</li>
<li><p>常见方法：在原模型优化目标的基础上，<strong>增加对参数的惩罚项(Penalty)</strong></p>
</li>
</ul>
<p>以最小二乘优化目标为例：</p>
<p>最小二乘优化目标: $argminL(w,b)=argmin\sum_{m}^{k=1}\qquad(f(w,x,b)-y^k)^2$</p>
<p>若加入对模型的<em>L1</em>范数正则化，则新的线性回归目标为：</p>
<p>即<strong>在原优化目标的基础上，增加了参数向量的L1范数，则在新目标优化过程中需要考虑L1惩罚项的影响</strong>。</p>
<p>为使目标最小化，这种正则化方法的结果是让参数向量中的许多元素趋向于0，使大部分特征失去对优化目标的贡献。而这种让有效特征变得稀疏的<em>L1</em>正则化模型，称为<em>Lasso</em>。</p>
<p><strong>Lasso模型在4次多项式特征上的拟合表现</strong></p>
<table>
<thead>
<tr>
<th>123456</th>
<th>from sklearn.linear_model import Lassolasso_poly4=Lasso()lasso_poly4.fit(X_train_poly4,y_train)print lasso_poly4.score(X_test_poly4,y_test)# 输出lasso模型的参数列表print lasso_poly4.coef_ # 回顾普通4次多项式回归模型拟合后的性能和参数列表print regressor_poly4.score(X_test_poly4,y_test)print regressor_poly4.coef_</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="L2范数正则化"><a href="#L2范数正则化" class="headerlink" title="L2范数正则化"></a>L2范数正则化</h3><p>与L1范数正则化略有不同，<strong>L2范数正则化在原优化目标上增加了参数向量的L2范数的惩罚项</strong>，公式如下：</p>
<p>为使新优化目标最小化，这种正则化方法的结果会让参数向量中的大部分元素都变得很小，压制了参数之间的差异性，这种<strong>压制参数间差异性</strong>的L2正则化模型被称为<em>Ridge</em>。</p>
<p><strong>Ridge模型在4次多项式特征上的拟合表现</strong></p>
<table>
<thead>
<tr>
<th>1234</th>
<th># 输出普通4次多项式回归模型的参数列表print regressor_poly4.coef_# 输出上述参数的平方和，验证参数间的巨大差异print np.sum(regressor_poly4.coef_ *<em>2) from sklearn.linear_model import Ridgeridge_poly4=Ridge()ridge_poly4.fit(X_train_poly4,y_train)print ridge_poly4.score(X_test_poly4,y_test)print ridge_poly4.coef_print np.sum(ridge_poly4.coef_ *</em>2)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>由输出可见，相比普通4次多项式回归模型，默认配置下的Ridge模型性能提高了约3%，且<strong>模型拟合后的参数间差异非常小</strong>。</p>
<p>λ是调节因子。</p>
<h3 id="模型检验"><a href="#模型检验" class="headerlink" title="模型检验"></a>模型检验</h3><p>错误的做法：拿测试集的正确结果反复调优模型与特征。</p>
<p>正确的做法：</p>
<ul>
<li>充分使用现有数据，对现有数据进行采样分割，一部分用于模型参数训练（训练集），另一部分用于调优模型配置和特征选择，并对未知测试性能作出估计（开发集Development Set/验证集Validation Set）</li>
</ul>
<p><strong>根据验证流程复杂度的不同</strong>，模型验证方式分为：留一验证 &amp; 交叉验证</p>
<p>3.1.3.1 留一验证(Leave-one-out cross validation)</p>
<p>留一验证</p>
<ul>
<li><p>最简单，即从任务提供的数据中，随机采样一定比例作为训练集，剩下的“留作”验证</p>
</li>
<li><p>通常比例为：7：3</p>
</li>
<li><p>缺点：模型性能不稳定（由于对验证集合随机采样的不确定性）</p>
</li>
<li><p>适用：计算能力较弱、而相对数据规模较大的机器学习发展早期（现在应该很少用了）</p>
</li>
</ul>
<p>3.1.3.2 交叉验证(K-fold cross-validation)</p>
<p>交叉验证</p>
<p><strong>执行了多次留一验证的过程</strong></p>
<p>每次检验所使用的<strong>验证集之间互斥</strong>，且需保证<strong>每一条可用数据都被模型验证过</strong></p>
<p>优点：<strong>保证所有数据都有被训练和验证的机会</strong>，尽最大可能让优化的模型性能表现<strong>可信</strong></p>
<ul>
<li><p>以5折(five-fold)交叉验证为例：</p>
</li>
<li><p>全部可用数据被随机分割为平均数量的5组，每次迭代都选取其中的1组数据作为验证集，其余4组作为训练集</p>
</li>
</ul>
<p>​         <img src="10.jpg" alt="img">       </p>
<h3 id="超参数搜索-hyperparameter"><a href="#超参数搜索-hyperparameter" class="headerlink" title="超参数搜索(hyperparameter)"></a>超参数搜索(hyperparameter)</h3><p><strong>超参数</strong></p>
<ul>
<li><p>指<strong>开始学习之前设置值的参数（模型配置）</strong>，而非通过训练得到的参数</p>
</li>
<li><p>如K近邻算法中的K值，SVM中不同的核函数(Kernal)</p>
</li>
<li><p>多数情况下，<strong>超参数的选择是无限的</strong>；故在有限时间内，除了可验证人工预设的几种超参数组合外，还可<strong>通过启发式的搜索方法对超参数组合进行调优</strong>，这种启发式的超参数搜索法称为<strong>网络搜索</strong></p>
</li>
<li><p>超参数的<strong>验证过程之间彼此独立</strong>，故适合<strong>并行计算</strong></p>
</li>
</ul>
<p>3.1.4.1 网格搜索(GridSearch)</p>
<p>由于超参数的空间无尽，故<strong>超参数组合配置只能是更优解，没有最优解</strong>。</p>
<p>网格搜索</p>
<ul>
<li><strong>依赖网格搜索对多种超参数组合的空间进行暴力搜索</strong>，<strong>每一套超参数组合被代入到学习函数中作为新的模型</strong>，且为比较新模型间的性能，<strong>每个模型都会采用交叉验证法在多组相同的训练和开发集下进行评估</strong></li>
</ul>
<p><strong>使用单线程对文本分类的Naive Bayes模型的超参数组合执行网格搜索</strong></p>
<table>
<thead>
<tr>
<th>12345</th>
<th>from sklearn.datasets import fetch_20newsgroupsimport numpy as npnews=fetch_20newsgroups(subset=’all’)print len(news.data)print news.data[0] from sklearn.cross_validation import train_test_split# 选取前3000条新闻文本进行分割X_train,X_test,y_train,y_test=train_test_split(news.data[:3000],news.target[:3000],test_size=0.25,random_state=33) from sklearn.svm import SVCfrom sklearn.feature_extraction.text import TfidfVectorizer # 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来from sklearn.pipeline import Pipelineclf=Pipeline([(‘vect’,TfidfVectorizer(stop_words=’english’,analyzer=’word’)),(‘svc’,SVC())]) # 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1…，则共有12种超参数组合，12种不同参数下的模型parameters={‘svc_gamma’:np.logspace(-2,1,4),’svc_C’:np.logspace(-1,1,3)} # 导入网络搜索模块GridSearchCVfrom sklearn.grid_search import GridSearchCV# 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定gs=GridSearchCV(clf,parameters,verbose=2,refit=True,cv=3) # 执行单线程网络搜索%time_=gs.fit(X_train,y_train)gs.best_params_,gs.best_score_ print gs.score(X_test,y_test)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>注：</p>
<ul>
<li><p><strong>np.logspace(a,b,c):创建等比数列</strong>，a为起始点，b为终点，c为元素个数;<strong>np.linspace(a,b,c):创建等差数列</strong></p>
</li>
<li><p><strong>refit=True</strong>: 使程序以交叉验证训练集得到的最佳超参数重新对所有可用的训练集合开发集进行，作为最终用于性能评估的最佳模型参数</p>
</li>
</ul>
<p>结果分析：</p>
<ul>
<li><p>使用单线程网格搜索技术对朴素贝叶斯模型在文本分类任务中的超参数组合进行调优，共有12组超参数X3折交叉验证=36项独立运行的计算任务。寻找到的最佳超参数组合在测试集上所能达成的最高分类准确性为82.27%。</p>
</li>
<li><p><strong>缺点：耗时</strong></p>
</li>
<li><p>优点：一旦获取到好超参数组合，则可以保持一段时间使用，是<strong>一劳永逸提高模型性能</strong>的方法</p>
</li>
</ul>
<p>3.1.4.2 并行搜索(Parallel Grid Search)</p>
<p>由于<strong>各新模型在执行交叉验证过程中相互独立</strong>，故我们可<strong>充分利用多核处理器甚至是分布式计算资源来从事并行搜索</strong>，则可成倍节省运算时间。</p>
<p><strong>使用多线程对文本分类的Naive Bayes模型的超参数组合执行并行化的网络搜索</strong></p>
<table>
<thead>
<tr>
<th>12345678910111213141516171819202122232425262728293031</th>
<th>from sklearn.datasets import fetch_20newsgroupsimport numpy as npnews=fetch_20newsgroups(subset=’all’) from sklearn.cross_validation import train_test_split# 选取前3000条新闻文本进行分割X_train,X_test,y_train,y_test=train_test_split(news.data[:3000],news.target[:3000],test_size=0.25,random_state=33) from sklearn.svm import SVCfrom sklearn.feature_extraction.text import TfidfVectorizer # 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来from sklearn.pipeline import Pipelineclf=Pipeline([(‘vect’,TfidfVectorizer(stop_words=’english’,analyzer=’word’)),(‘svc’,SVC())]) # 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1…，则共有12种超参数组合，12种不同参数下的模型parameters={‘svc_gamma’:np.logspace(-2,1,4),’svc_C’:np.logspace(-1,1,3)} # 导入网络搜索模块GridSearchCVfrom sklearn.grid_search import GridSearchCV # 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定# 初始化配置并行网络搜索，n_jobs=-1代表使用该计算机的全部CPUgs=GridSearchCV(clf,parameters,verbose=2,refit=True,cv=3,n_jobs=-1) # 执行多线程并行网络搜索%time_=gs.fit(X_train,y_train)gs.best_params_,gs.best_score_ # 输出最佳模型在测试集上的准确性print gs.score(X_test,y_test)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>结果分析：</p>
<ul>
<li><p>相较于单线程，多线程的计算时间仅为51.8秒，且准确性仍为82.27%</p>
</li>
<li><p><strong>并行搜索：有效利用多核处理器的计算资源，几乎成倍提升运算速度，节省最佳超参数组合的搜索时间</strong></p>
</li>
</ul>
<h2 id="流行库-模型实践"><a href="#流行库-模型实践" class="headerlink" title="流行库/模型实践"></a>流行库/模型实践</h2><h3 id="NLTK自然语言处理包"><a href="#NLTK自然语言处理包" class="headerlink" title="NLTK自然语言处理包"></a>NLTK自然语言处理包</h3><p>计算语言学</p>
<ul>
<li><p>借助计算机强大的运算能力和海量的互联网文本，来提高自然语言处理能力</p>
</li>
<li><p>如何让计算机处理、生成、理解人类的自然语言</p>
</li>
</ul>
<p>NLTK(Natural Language Toolkit)</p>
<ul>
<li>高效的语言学家，快速完成对自然语言文本的深层处理和分析</li>
</ul>
<p><strong>1.使用词袋法(bag of words)对文本进行特征向量化</strong></p>
<table>
<thead>
<tr>
<th>12345678910111213</th>
<th>sent1=’The dog is walking on the street.’sent2=’A cat was running across the room.’ from sklearn.feature_extraction.text import CountVectorizercvr=CountVectorizer()sent=[sent1,sent2]print cvr.fit_transform(sent).toarray()print cvr.get_feature_names() from sklearn.feature_extraction.text import TfidfVectorizertfidf=TfidfVectorizer()print tfidf.fit_transform(sent).toarray()print tfidf.get_feature_names()</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>2.使用NLTK对文本进行语言学分析</strong></p>
<table>
<thead>
<tr>
<th>1234567891011121314</th>
<th>import nltk # 对句子进行分词和正规化tokens_1=nltk.word_tokenize(sent1)print tokens_1 # 初始化词性标注器，对每个词汇进行标注pos_tag_1=nltk.tag.pos_tag(tokens_1)print pos_tag_1 # 初始化stemmer，寻找各个词汇最原始的词根stemmer=nltk.stem.PorterStemmer()stem_1=[stemmer.stem(t) for t in tokens_1]print stem_1</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="词向量技术-Word2Vec"><a href="#词向量技术-Word2Vec" class="headerlink" title="词向量技术(Word2Vec)"></a>词向量技术(Word2Vec)</h3><p><strong>词袋法可对文本向量化，但无法计算两段文本间的相似性</strong>。如sent1与sent2在词袋法看来唯一相同的词汇是the，找不到任何语义层面的联系。</p>
<p><strong>Word2Vec模型</strong>：</p>
<ul>
<li><p>用来<strong>产生词向量</strong>的相关模型，为<strong>浅层神经网络，是监督学习系统</strong></p>
</li>
<li><p>网络以词表现，并需猜测相邻位置的输入词；训练完成后，Word2vec模型可用来映射每个词到一个向量，表示词对词之间的关系（该向量为神经网络之隐藏层）</p>
</li>
<li><p><strong>词汇间的联系通过上下文(context)建立</strong></p>
</li>
<li><p>如”A cat was running across the room.”,若需要上下文数量为4的连续词汇片段，则有A cat was running、cat was running across、was running across the、running across the room. 每个连续词汇片段的最后一个单词有可能是什么都是受到前面3个词汇的制约，故形成由前3个词汇预测最后一个单词的监督学习系统</p>
</li>
<li><p>当上下文数量为n时，提供给网络的输入(Input)都是前n-1个连续词汇$w<em>{t-m+1},…,w</em>{t-1}$，指向的目标输出(Output)就是最后一个单词$w<em>{t}$；而在网络中用于计算的都是这些词汇的向量表示，如$C(w</em>{t-1})$，每一个实心红色圆都代表词向量中的元素；每个词汇红色实心圆的个数代表词向量的维度(dimension),且所有词向量的维度一致。神经网络的训练也是一个通过不断迭代、更新参数循环往复的过程，<strong>从网络中最终获得的即每个词汇独特的向量表示</strong>。</p>
</li>
</ul>
<p>​         <img src="11.jpg" alt="img">       </p>
<h3 id="XGBoost模型-extreme-gradient-boosting"><a href="#XGBoost模型-extreme-gradient-boosting" class="headerlink" title="XGBoost模型(extreme gradient boosting)"></a>XGBoost模型(extreme gradient boosting)</h3><p>XGBoost模型</p>
<ul>
<li><p>提升(Boosting)分类器隶属于集成学习模型，基本思想：把成千上万个分类准确率较低的树模型组合起来，成为一个准确率很高的模型</p>
</li>
<li><p>特点：不断迭代，每次迭代都生成一棵新的树;能自动利用CPU的多线程进行并行计算</p>
</li>
<li><p>如何生成合理的树？如梯度提升树</p>
</li>
</ul>
<p><strong>实践：对比随机决策森林和XGBoost模型对titanic上的乘客是否生还的预测能力</strong></p>
<table>
<thead>
<tr>
<th>12345678910111213141516171819202122232425262728293031323334</th>
<th># coding:utf-8import pandas as pd titanic=pd.read_csv(‘/Users/scarlett/repository/projects/titanic/titanic.csv’)print (titanic.info()) # 选取训练特征X=titanic[[‘pclass’,’age’,’sex’]]y=titanic[‘survived’] # 补全缺失值X[‘age’].fillna(X[‘age’].mean(),inplace=True) # 数据分割from sklearn.cross_validation import train_test_splitX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=33) # 导入文本特征向量化的模块DictVectorizerfrom sklearn.feature_extraction import DictVectorizervec=DictVectorizer(sparse=False)# 对原数据进行特征向量化处理X_train=vec.fit_transform(X_train.to_dict(orient=’record’))X_test=vec.transform(X_test.to_dict(orient=’record’)) # 采用默认配置下的随机森林分类器对测试集进行预测from sklearn.ensemble import RandomForestClassifierrfc=RandomForestClassifier()rfc.fit(X_train,y_train)print (‘rfc accuracy:’,rfc.score(X_test,y_test)) # 采用默认配置的xgboost模型对相同测试集进行预测from xgboost import XGBClassifierxgbc=XGBClassifier()xgbc.fit(X_train,y_train)print ‘xgbc accuracy:’,xgbc.score(X_test,y_test)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="TensorFlow框架"><a href="#TensorFlow框架" class="headerlink" title="TensorFlow框架"></a>TensorFlow框架</h3><p>TensorFlow</p>
<ul>
<li><p>一个完整的编码框架</p>
</li>
<li><p>使用图(Graph)来表示计算任务，使用会话(Session)来执行图</p>
</li>
</ul>
<p><strong>1.使用TensorFlow输出一句话</strong></p>
<table>
<thead>
<tr>
<th>123456789101112131415</th>
<th># 使用TensorFlow输出一句话 import tensorflow as tf import numpy as np  # 初始化一个TensorFlow常量，使greeting作为一个计算模块greeting=tf.constant(“Hello Google!”)# 启动一个会话sess=tf.Session()# 使用会话执行greeting计算模块result=sess.run(greeting)# 输出会话执行结果print result# 关闭会话sess.close()</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>2.使用TensorFlow完成一次线性函数的运算</strong></p>
<table>
<thead>
<tr>
<th>12345678910111213</th>
<th># 使用TensorFlow完成一次线性函数的运算 # 声明matrix1为一个1<em>2的行向量，matrix2为一个2</em>1的列向量matrix1=tf.constant([[3.,3.]])matrix2=tf.constant([[2.],[2.]])# Product将上面两个算子相乘，作为新算例product=tf.matmul(matrix1, matrix2)# 继续讲Product与一个标量2.0求和拼接，作为最终linear算例linear=tf.add(product, tf.constant(2.0))# 直接在会话中执行linear算例，相当于将上面所有单独的算例拼接成流程图来执行with tf.Session() as sess:    result=sess.run(linear)    print result</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>Skflow</p>
<ul>
<li>sklearn X tensorflow</li>
</ul>
<p>人工神经网络(Artificial Neural Network)</p>
<ul>
<li>神经信息传递的大致工作原理：神经元的树突(Dendrite)接收其他神经元传递过来的信息，然后神经细胞体对信息进行加工后，会通过轴突(Axon)把加工后的信息传递到轴突终端(Axon Terminal)然后再传递给其他神经元的树突。如此，则大量神经元就连接成了一个结构复杂的神经网络。</li>
</ul>
<p>感知机</p>
<ul>
<li><p>模拟神经元</p>
</li>
<li><p>n维输入信号(Input) $x=$，对应的参数向量$w=$，和截距b，输出信号y(Output)等</p>
</li>
<li><p>感知机在具体运算上采用线性加权求和的方式处理输入信号，即</p>
</li>
</ul>
<p>为模拟神经元行为，定义如下激活(符号)函数，由此可知感知机最终会产生两种离散数值的输出(output)信号:</p>
<ul>
<li>感知机模型的最大贡献在于：设计了一套算法使模型可<strong>通过不断根据训练数据更新参数，达到具备线性二分类模型的学习能力</strong></li>
</ul>
<p>多层感知机(Multi-layer Perceptrons人工神经网络)</p>
<p><strong>实践：使用skflow内置的LinearRegressor、DNN和Sciki-learn中的集成回归模型对“美国波士顿房价”数据进行回归预测</strong></p>
<table>
<thead>
<tr>
<th>1234567891011121314151617181920212223242526272829303132333435363738</th>
<th>from sklearn import datasets,metrics,preprocessing,cross_validationboston=datasets.load_boston()X,y=boston.data,boston.target X_train,X_test,y_train,y_test=cross_validation.train_test_split(X,y,test_size=0.25,random_state=33)scaler=preprocessing.StandardScaler()X_train=scaler.fit_transform(X_train)X_test=scaler.transform(X_test) import skflowtf_lr=skflow.TensorFlowLinearRegressor(steps=10000,learning_rate=0.01,batch_size=50)tf_lr.fit(X_train, y_train)tf_lr_y_predict=tf_lr.predict(X_test) # 输出skflow中linearregressor模型的回归性能print ‘MAE:’,metrics.mean_absolute_error(tf_lr_y_predict, y_test)print ‘MSE:’,metrics.mean_squared_error(tf_lr_y_predict, y_test)print ‘R-squared:’,metrics.r2_score(tf_lr_y_predict, y_test) # 使用skflow的DNNRegressor，并注意其每个隐层特征数量的配置tf_dnn_regressor=skflow.TensorFlowDNNRegressor(hidden_units=[100,40],    steps=10000,learning_rate=0.01,batch_size=50)tf_dnn_regressor.fit(X_train,y_train)tf_dnn_regressor_y_predict=tf_dnn_regressor.predict(X_test) # 输出skflow中DNNRegressor模型的回归性能print ‘MSE dnn:’,metrics.mean_squared_error(tf_dnn_regressor_y_predict, y_test)print ‘MAE dnn:’,metrics.mean_absolute_error(tf_dnn_regressor_y_predict, y_test)print ‘R-squared dnn:’,metrics.r2_score(tf_dnn_regressor_y_predict, y_test) # 使用sklearn中的RandomForestRegressorfrom sklearn.ensemble import RandomForestRegressorrfr=RandomForestRegressor()rfr.fit(X_train,y_train)rfr_y_predict=rfr.predict(X_test)print ‘MSE rfr:’,metrics.mean_squared_error(rfr_y_predict, y_test)print ‘MAE rfr:’,metrics.mean_absolute_error(rfr_y_predict, y_test)print ‘R-squared rfr:’,metrics.r2_score(rfr_y_predict, y_test)</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>由输出可知：深度神经网络性能更佳。但需要注意：</p>
<ul>
<li>越是具备描述复杂数据的强力模型，越容易在训练时陷入过拟合，这一点需要在配置DNN的层数和每层特征元的数量时特别注意。</li>
</ul>
<h1 id="第四章-实战篇"><a href="#第四章-实战篇" class="headerlink" title="第四章 实战篇"></a>第四章 实战篇</h1><h2 id="Kaggle"><a href="#Kaggle" class="headerlink" title="Kaggle"></a>Kaggle</h2><p>Kaggle是目前世界上最流行的，采用众包策略(Crowdsouring)，为科技公司、研究院所乃至高校课程提供数据分析和预测模型的竞赛平台。</p>
<p>平台宗旨: 汇聚全世界从事数据分析和预测的专家和兴趣爱好者的集体智慧，利用公开数据竞赛的方式，为科技公司、研究院所和高校课程中的研发课题，提供有效的解决方案。使问题提出者与解决者获得双赢。</p>
<ul>
<li><p>问题提出者：支付少量奖金即可获得全世界聪明人的解决方案</p>
</li>
<li><p>解决者：获得大量可供分析的真实业务数据</p>
</li>
</ul>
<p>参与流程</p>
<ul>
<li><p>下载数据(download)</p>
</li>
<li><p>搭建模型(build)</p>
</li>
<li><p>提交结果(submit)</p>
</li>
</ul>
<h2 id="Titanic罹难乘客预测竞赛编码示例"><a href="#Titanic罹难乘客预测竞赛编码示例" class="headerlink" title="Titanic罹难乘客预测竞赛编码示例"></a>Titanic罹难乘客预测竞赛编码示例</h2><ul>
<li><p>导入pandas方便数据读取和预处理</p>
</li>
<li><p>分别对训练和测试数据从本地进行读取</p>
</li>
<li><p>先分别输出训练与测试数据的基本信息。对数据情况有一个大致的了解</p>
</li>
<li><p>根据经验人工选取对预测有效的特征</p>
</li>
<li><p>补完特征的缺失值</p>
</li>
<li><p>重新对处理后的训练和测试数据进行查验</p>
</li>
<li><p>用DictVectorizer对特征向量化</p>
</li>
<li><p>选择模型进行训练</p>
</li>
<li><p>使用5折交叉验证的方法在训练集上分别对默认配置的两个模型进行性能评估，并获得平均分类准确性的得分</p>
</li>
<li><p>将测试数据的预测结果存储在csv文件中</p>
</li>
<li><p>使用并行网格搜索的方式寻找更好的超参数组合，以期待进一步提高模型的预测性能</p>
</li>
<li><p>查验优化之后的模型的超参数配置以及交叉验证的准确性</p>
</li>
<li><p>使用经过优化超参数配置的模型对测试数据的预测结果存储在csv文件中</p>
</li>
</ul>
<h2 id="IMDB影评得分估计竞赛编码示例"><a href="#IMDB影评得分估计竞赛编码示例" class="headerlink" title="IMDB影评得分估计竞赛编码示例"></a>IMDB影评得分估计竞赛编码示例</h2><ul>
<li><p>导入pandas用于读取和写入数据操作</p>
</li>
<li><p>从本地读入训练与测试数据集</p>
</li>
<li><p>查验一下前几条数据</p>
</li>
<li><p>导入BeautifulSoup用于整洁原始文本</p>
</li>
<li><p>导入正则表达式工具包</p>
</li>
<li><p>从nltk.corpus里导入停用词列表</p>
</li>
<li><p>完成对原始评论的三项数据预处理任务（去掉html标记、去掉非字母字符、进一步去掉评论中的停用词）</p>
</li>
<li><p>导入文本特征抽取器CountVectorizer、TfidVectorizer</p>
</li>
<li><p>从scikit-learn中导入朴素贝叶斯模型</p>
</li>
<li><p>导入Pipeline用于方便搭建系统流程</p>
</li>
<li><p>导入GridSearchCV用于超参数组合的网格搜索</p>
</li>
<li><p>使用Pipeline搭建两组使用朴素贝叶斯模型的分类器，区别在于分别使用两种文本特征抽取器对文本特征进行抽取</p>
</li>
<li><p>分别配置用于模型超参数搜索的组合</p>
</li>
<li><p>使用采用4折交叉验证的方法对使用countvectorizer的朴素贝叶斯模型进行并行化超参数搜索</p>
</li>
<li><p>输出交叉验证中最佳的准确性得分以及超参数组合</p>
</li>
<li><p>对采用tfidfvectorizer的模型重复上两步</p>
</li>
<li><p>以最佳的超参数组合配置模型并对测试数据进行预测</p>
</li>
<li><p>使用pandas对需要提交的数据进行格式化</p>
</li>
<li><p>结果输出到本地硬盘</p>
</li>
<li><p>从本地读入未标记数据</p>
</li>
<li><p>导入nltk.data</p>
</li>
<li><p>准备使用nltk的tokenizer对影评中的英文句子进行分割</p>
</li>
<li><p>定义函数review_to_sentences逐条对影评进行分句</p>
</li>
<li><p>准备用于训练词向量的数据</p>
</li>
<li><p>配置训练词向量模型的超参数</p>
</li>
<li><p>从gensim.models导入word2vec</p>
</li>
<li><p>开始词向量模型的训练</p>
</li>
<li><p>可以将词向量模型的训练结果长期保存于本地硬盘</p>
</li>
<li><p>直接读入已经训练好的词向量模型</p>
</li>
<li><p>探查一下该词向量模型的训练成果</p>
</li>
<li><p>定义一个函数使用词向量产生文本特征向量</p>
</li>
<li><p>定义另一个每条影评转化为基于词向量的特征向量（平均词向量）</p>
</li>
<li><p>准备新的基于词向量表示的训练和测试特征向量</p>
</li>
<li><p>从sklearn.ensemble导入GradientBoostingClassifier模型进行影评情感分析</p>
</li>
<li><p>从sklearn.grid_search导入GridSearchCV用于超参数的网格搜索</p>
</li>
<li><p>配置超参数的搜索组合</p>
</li>
<li><p>输出网格搜索得到的最佳性能以及最优超参数组合</p>
</li>
<li><p>使用超参数调优之后的梯度上升树模型进行预测</p>
</li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Alfred Chen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://alfredchen.cn/2019/03/14/%E3%80%8APython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/" title="《Python机器学习及实战》笔记">http://alfredchen.cn/2019/03/14/《Python机器学习及实战》笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/03/10/%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E3%80%81F1%E5%80%BC%E8%A7%A3%E6%9E%90/" rel="prev" title="准确率、精确率、召回率、F1值解析">
      <i class="fa fa-chevron-left"></i> 准确率、精确率、召回率、F1值解析
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/03/20/%E9%A3%8E%E6%8E%A7%E5%AE%89%E5%85%A8%E5%85%A5%E9%97%A8/" rel="next" title="风控安全入门">
      风控安全入门 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第一章-简介篇"><span class="nav-number">1.</span> <span class="nav-text">第一章 简介篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习综述"><span class="nav-number">1.1.</span> <span class="nav-text">机器学习综述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#任务"><span class="nav-number">1.2.</span> <span class="nav-text">任务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习：关注对事物未知表现的预测"><span class="nav-number">1.2.1.</span> <span class="nav-text">监督学习：关注对事物未知表现的预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#无监督学习：倾向于对事物本身特性的分析"><span class="nav-number">1.2.2.</span> <span class="nav-text">无监督学习：倾向于对事物本身特性的分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#经验"><span class="nav-number">1.3.</span> <span class="nav-text">经验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#性能"><span class="nav-number">1.4.</span> <span class="nav-text">性能</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#python编程库"><span class="nav-number">2.</span> <span class="nav-text">python编程库</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#python用于机器学习的优势"><span class="nav-number">2.1.</span> <span class="nav-text">python用于机器学习的优势</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python编程库-1"><span class="nav-number">2.2.</span> <span class="nav-text">python编程库</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第二章-基础篇"><span class="nav-number">3.</span> <span class="nav-text">第二章 基础篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#监督学习经典模型"><span class="nav-number">3.1.</span> <span class="nav-text">监督学习经典模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监督学习的流程"><span class="nav-number">3.1.1.</span> <span class="nav-text">监督学习的流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分类学习"><span class="nav-number">3.2.</span> <span class="nav-text">分类学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性分类器（Linear-Classifier）"><span class="nav-number">3.2.1.</span> <span class="nav-text">线性分类器（Linear Classifier）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量机（Support-Vector-Classifier）"><span class="nav-number">3.2.2.</span> <span class="nav-text">支持向量机（Support Vector Classifier）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯（Naive-Bayes）"><span class="nav-number">3.2.3.</span> <span class="nav-text">朴素贝叶斯（Naive Bayes）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K近邻（K-Nearest-Neighbor，KNN）"><span class="nav-number">3.2.4.</span> <span class="nav-text">K近邻（K-Nearest Neighbor，KNN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树（Decision-Tree）"><span class="nav-number">3.2.5.</span> <span class="nav-text">决策树（Decision Tree）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集成模型（Ensemble）"><span class="nav-number">3.2.6.</span> <span class="nav-text">集成模型（Ensemble）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#回归预测"><span class="nav-number">3.3.</span> <span class="nav-text">回归预测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归器"><span class="nav-number">3.3.1.</span> <span class="nav-text">线性回归器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量机"><span class="nav-number">3.3.2.</span> <span class="nav-text">支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K近邻"><span class="nav-number">3.3.3.</span> <span class="nav-text">K近邻</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回归树"><span class="nav-number">3.3.4.</span> <span class="nav-text">回归树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#集成模型"><span class="nav-number">3.3.5.</span> <span class="nav-text">集成模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#无监督学习经典模型"><span class="nav-number">3.4.</span> <span class="nav-text">无监督学习经典模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据聚类"><span class="nav-number">3.5.</span> <span class="nav-text">数据聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means算法"><span class="nav-number">3.5.1.</span> <span class="nav-text">K-means算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#null"><span class="nav-number">4.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征降维"><span class="nav-number">4.1.</span> <span class="nav-text">特征降维</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#主成分分析（Principle-Component-Analysis）"><span class="nav-number">4.1.1.</span> <span class="nav-text">主成分分析（Principle Component Analysis）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第三章-进阶篇"><span class="nav-number">5.</span> <span class="nav-text">第三章 进阶篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#模型实用技巧"><span class="nav-number">5.1.</span> <span class="nav-text">模型实用技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#特征提升"><span class="nav-number">5.1.1.</span> <span class="nav-text">特征提升</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型正则化"><span class="nav-number">5.1.2.</span> <span class="nav-text">模型正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L2范数正则化"><span class="nav-number">5.1.3.</span> <span class="nav-text">L2范数正则化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型检验"><span class="nav-number">5.1.4.</span> <span class="nav-text">模型检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#超参数搜索-hyperparameter"><span class="nav-number">5.1.5.</span> <span class="nav-text">超参数搜索(hyperparameter)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#流行库-模型实践"><span class="nav-number">5.2.</span> <span class="nav-text">流行库&#x2F;模型实践</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#NLTK自然语言处理包"><span class="nav-number">5.2.1.</span> <span class="nav-text">NLTK自然语言处理包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#词向量技术-Word2Vec"><span class="nav-number">5.2.2.</span> <span class="nav-text">词向量技术(Word2Vec)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#XGBoost模型-extreme-gradient-boosting"><span class="nav-number">5.2.3.</span> <span class="nav-text">XGBoost模型(extreme gradient boosting)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow框架"><span class="nav-number">5.2.4.</span> <span class="nav-text">TensorFlow框架</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第四章-实战篇"><span class="nav-number">6.</span> <span class="nav-text">第四章 实战篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kaggle"><span class="nav-number">6.1.</span> <span class="nav-text">Kaggle</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Titanic罹难乘客预测竞赛编码示例"><span class="nav-number">6.2.</span> <span class="nav-text">Titanic罹难乘客预测竞赛编码示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IMDB影评得分估计竞赛编码示例"><span class="nav-number">6.3.</span> <span class="nav-text">IMDB影评得分估计竞赛编码示例</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Alfred Chen"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Alfred Chen</p>
  <div class="site-description" itemprop="description">对不合理之处保持敏感</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/cemeworm" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;cemeworm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:cemeworm@gmail.com" title="E-Mail → mailto:cemeworm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.jianshu.com/u/6f99e4cac30a" title="jianshu → https:&#x2F;&#x2F;www.jianshu.com&#x2F;u&#x2F;6f99e4cac30a" rel="noopener" target="_blank"><i class="fa fa-fw fa-map"></i>jianshu</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://www.woshipm.com/u/333150" title="woshipm → http:&#x2F;&#x2F;www.woshipm.com&#x2F;u&#x2F;333150" rel="noopener" target="_blank"><i class="fa fa-fw fa-renren"></i>woshipm</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alfred Chen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">210k</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.css">
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: false,
      notify: false,
      appId: 'frczYHpH6uvUJGbY8QEQcbT0-gzGzoHsz',
      appKey: 'tnCTL5tBVAVki6JejiSIYqbu',
      placeholder: "快来评论一下吧~",
      avatar: 'monsterid',
      meta: guest,
      pageSize: '10' || 10,
      visitor: false,
      lang: '' || 'zh-cn',
      path: location.pathname,
      recordIP: false,
      serverURLs: ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
